{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyuzdRWJuRzd"
      },
      "source": [
        "# Access and Push"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t75b2qYBy1xA",
        "outputId": "88dad9e6-a0e3-4a6a-a93e-425f101fb0e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Cloning into 'Capstone-Tang'...\n",
            "remote: Enumerating objects: 7088, done.\u001b[K\n",
            "remote: Counting objects: 100% (6/6), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 7088 (delta 1), reused 0 (delta 0), pack-reused 7082 (from 1)\u001b[K\n",
            "Receiving objects: 100% (7088/7088), 1.09 GiB | 56.24 MiB/s, done.\n",
            "Resolving deltas: 100% (7/7), done.\n",
            "Updating files: 100% (12943/12943), done.\n"
          ]
        }
      ],
      "source": [
        "# Access my drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Access github\n",
        "!git clone "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttpFAcL6zFZF"
      },
      "source": [
        "Git push:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OGibWqT6ug_O"
      },
      "outputs": [],
      "source": [
        "%cd /content/Capstone-Tang\n",
        "!git status"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "ThVMQ3chzBmN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87db8f95-9302-4abe-b677-a9c0a38baaa1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "On branch main\n",
            "Your branch is ahead of 'origin/main' by 1 commit.\n",
            "  (use \"git push\" to publish your local commits)\n",
            "\n",
            "nothing to commit, working tree clean\n",
            "Enumerating objects: 19038, done.\n",
            "Counting objects: 100% (19038/19038), done.\n",
            "Delta compression using up to 8 threads\n",
            "Compressing objects: 100% (18675/18675), done.\n",
            "error: RPC failed; HTTP 500 curl 22 The requested URL returned error: 500\n",
            "send-pack: unexpected disconnect while reading sideband packet\n",
            "Writing objects: 100% (19034/19034), 3.24 GiB | 76.61 MiB/s, done.\n",
            "Total 19034 (delta 16), reused 19033 (delta 15), pack-reused 0\n",
            "fatal: the remote end hung up unexpectedly\n",
            "Everything up-to-date\n"
          ]
        }
      ],
      "source": [
        "!git add .\n",
        "# !git config http.postBuffer 524288000\n",
        "# !git config http.maxRequestBuffer 100M\n",
        "!git config --global user.email \"tianyitang666@gmail.com\"\n",
        "!git config --global user.name \"floragreen666\"\n",
        "!git commit -m \"\"  # don't forget commit message\n",
        "!git push --no-thin origin main"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PO2VDLNHJdqP"
      },
      "source": [
        "# MOSEI Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_56atPMDLF3c"
      },
      "source": [
        "Since the goal of this capstone is to transcribe audios and do sentiment analysis on transcripts, we'll need a multimodal sentiment dataset that contains both audios, transcripts, and sentiment labels.\n",
        "\n",
        "MOSEI is a large-scale dataset with diverse, spontaneous spoken content from online videos. It includes transcriptions, which can be used to fine-tune transcription models. It also includes sentiment and emotion labels, which can be used to train sentiment analysis models on transcripts.\n",
        "\n",
        "The original paper of MOSEI can be found here: https://aclanthology.org/P18-1208/\n",
        "\n",
        "I also considered several other datasets: MELD is limited to the TV show \"Friends\", which may be difficult to generalize; IEMOCAP has limited number of actors and features acted emotions, which may not reflect diverse and spontaneous speech (same for CREMA-D and SAVEE); MOSI is similar to MOSEI, but not as comprehensive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tI63wZFJpBr"
      },
      "source": [
        "# Transfer Videos to Audios"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rzZHRDHKQ6Q"
      },
      "source": [
        "I tried to access the dataset through its official repo, yet following its instructions results in errors. Then I looked through a range of repos and datasets, gotten several pkl files which contain features for the video and audios of the dataset. I tried to decode them, but there were no instrctions on the form of the feature files, which makes it hard.\n",
        "\n",
        "I finally came across a repo that has a google drive link to the raw videos of the dataset: https://drive.google.com/drive/folders/1o2pOWQg8fxJkgBJVWk9mjCrbcc1jX4eq\n",
        "\n",
        "In this part, I would transfer more than 22000 videos to audios."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "DW5L1xCVJ7XC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import subprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ZWyo1ahgJ8Ei"
      },
      "outputs": [],
      "source": [
        "# Paths\n",
        "input_csv_path = '/content/Capstone-Tang/MOSEI/label.csv'\n",
        "video_base_path = '/content/drive/MyDrive/MOSEI/Raw'\n",
        "audio_base_path = '/content/drive/MyDrive/MOSEI/Audio'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zFzWIRMnDr-W",
        "outputId": "7ac6ca34-b3d3-4121-ced3-3de644c728cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 0 rows\n",
            "Processed 100 rows\n",
            "Processed 200 rows\n",
            "Processed 300 rows\n",
            "Processed 400 rows\n",
            "Processed 500 rows\n",
            "Processed 600 rows\n",
            "Processed 700 rows\n",
            "Processed 800 rows\n",
            "Processed 900 rows\n",
            "Processed 1000 rows\n",
            "Processed 1100 rows\n",
            "Processed 1200 rows\n",
            "Processed 1300 rows\n",
            "Processed 1400 rows\n",
            "Processed 1500 rows\n",
            "Processed 1600 rows\n",
            "Processed 1700 rows\n",
            "Processed 1800 rows\n",
            "Processed 1900 rows\n",
            "Processed 2000 rows\n",
            "Processed 2100 rows\n",
            "Processed 2200 rows\n",
            "Processed 2300 rows\n",
            "Processed 2400 rows\n",
            "Processed 2500 rows\n",
            "Processed 2600 rows\n",
            "Processed 2700 rows\n",
            "Processed 2800 rows\n",
            "Processed 2900 rows\n",
            "Processed 3000 rows\n",
            "Processed 3100 rows\n",
            "Processed 3200 rows\n",
            "Processed 3300 rows\n",
            "Processed 3400 rows\n",
            "Processed 3500 rows\n",
            "Processed 3600 rows\n",
            "Processed 3700 rows\n",
            "Processed 3800 rows\n",
            "Processed 3900 rows\n",
            "Processed 4000 rows\n",
            "Processed 4100 rows\n",
            "Processed 4200 rows\n",
            "Processed 4300 rows\n",
            "Processed 4400 rows\n",
            "Processed 4500 rows\n",
            "Processed 4600 rows\n",
            "Processed 4700 rows\n",
            "Processed 4800 rows\n",
            "Processed 4900 rows\n",
            "Processed 5000 rows\n",
            "Processed 5100 rows\n",
            "Processed 5200 rows\n",
            "Processed 5300 rows\n",
            "Processed 5400 rows\n",
            "Processed 5500 rows\n",
            "Processed 5600 rows\n",
            "Processed 5700 rows\n",
            "Processed 5800 rows\n",
            "Processed 5900 rows\n",
            "Processed 6000 rows\n",
            "Processed 6100 rows\n",
            "Processed 6200 rows\n",
            "Processed 6300 rows\n",
            "Processed 6400 rows\n",
            "Processed 6500 rows\n",
            "Processed 6600 rows\n",
            "Processed 6700 rows\n",
            "Processed 6800 rows\n",
            "Processed 6900 rows\n",
            "Processed 7000 rows\n",
            "Processed 7100 rows\n",
            "Processed 7200 rows\n",
            "Processed 7300 rows\n",
            "Processed 7400 rows\n",
            "Processed 7500 rows\n",
            "Processed 7600 rows\n",
            "Processed 7700 rows\n",
            "Processed 7800 rows\n",
            "Processed 7900 rows\n",
            "Processed 8000 rows\n",
            "Processed 8100 rows\n",
            "Processed 8200 rows\n",
            "Processed 8300 rows\n",
            "Processed 8400 rows\n",
            "Processed 8500 rows\n",
            "Processed 8600 rows\n",
            "Processed 8700 rows\n",
            "Processed 8800 rows\n",
            "Processed 8900 rows\n",
            "Processed 9000 rows\n",
            "Processed 9100 rows\n",
            "Processed 9200 rows\n",
            "Processed 9300 rows\n",
            "Processed 9400 rows\n",
            "Processed 9500 rows\n",
            "Processed 9600 rows\n",
            "Processed 9700 rows\n",
            "Processed 9800 rows\n",
            "Processed 9900 rows\n",
            "Processed 10000 rows\n",
            "Processed 10100 rows\n",
            "Processed 10200 rows\n",
            "Processed 10300 rows\n",
            "Processed 10400 rows\n",
            "Processed 10500 rows\n",
            "Processed 10600 rows\n",
            "Processed 10700 rows\n",
            "Processed 10800 rows\n",
            "Processed 10900 rows\n",
            "Processed 11000 rows\n",
            "Processed 11100 rows\n",
            "Processed 11200 rows\n",
            "Processed 11300 rows\n",
            "Processed 11400 rows\n",
            "Processed 11500 rows\n",
            "Processed 11600 rows\n",
            "Processed 11700 rows\n",
            "Processed 11800 rows\n",
            "Processed 11900 rows\n",
            "Processed 12000 rows\n",
            "Processed 12100 rows\n",
            "Processed 12200 rows\n",
            "Processed 12300 rows\n",
            "Processed 12400 rows\n",
            "Processed 12500 rows\n",
            "Processed 12600 rows\n",
            "Processed 12700 rows\n",
            "Processed 12800 rows\n",
            "Processed 12900 rows\n",
            "Processed 13000 rows\n",
            "Processed 13100 rows\n",
            "Processed 13200 rows\n",
            "Processed 13300 rows\n",
            "Processed 13400 rows\n",
            "Processed 13500 rows\n",
            "Processed 13600 rows\n",
            "Processed 13700 rows\n",
            "Processed 13800 rows\n",
            "Processed 13900 rows\n",
            "Processed 14000 rows\n",
            "Processed 14100 rows\n",
            "Processed 14200 rows\n",
            "Processed 14300 rows\n",
            "Processed 14400 rows\n",
            "Processed 14500 rows\n",
            "Processed 14600 rows\n",
            "Processed 14700 rows\n",
            "Processed 14800 rows\n",
            "Processed 14900 rows\n",
            "Processed 15000 rows\n",
            "Processed 15100 rows\n",
            "Processed 15200 rows\n",
            "Processed 15300 rows\n",
            "Processed 15400 rows\n",
            "Processed 15500 rows\n",
            "Processed 15600 rows\n",
            "Processed 15700 rows\n",
            "Processed 15800 rows\n",
            "Processed 15900 rows\n",
            "Processed 16000 rows\n",
            "Processed 16100 rows\n",
            "Processed 16200 rows\n",
            "Processed 16300 rows\n",
            "Processed 16400 rows\n",
            "Processed 16500 rows\n",
            "Processed 16600 rows\n",
            "Processed 16700 rows\n",
            "Processed 16800 rows\n",
            "Processed 16900 rows\n",
            "Processed 17000 rows\n",
            "Processed 17100 rows\n",
            "Processed 17200 rows\n",
            "Processed 17300 rows\n",
            "Processed 17400 rows\n",
            "Processed 17500 rows\n",
            "Processed 17600 rows\n",
            "Processed 17700 rows\n",
            "Processed 17800 rows\n",
            "Processed 17900 rows\n",
            "Processed 18000 rows\n",
            "Processed 18100 rows\n",
            "Processed 18200 rows\n",
            "Processed 18300 rows\n",
            "Processed 18400 rows\n",
            "Processed 18500 rows\n",
            "Processed 18600 rows\n",
            "Processed 18700 rows\n",
            "Processed 18800 rows\n",
            "Processed 18900 rows\n",
            "Processed 19000 rows\n",
            "Processed 19100 rows\n",
            "Processed 19200 rows\n",
            "Processed 19300 rows\n",
            "Processed 19400 rows\n",
            "Processed 19500 rows\n",
            "Processed 19600 rows\n",
            "Processed 19700 rows\n",
            "Processed 19800 rows\n",
            "Processed 19900 rows\n",
            "Processed 20000 rows\n",
            "Processed 20100 rows\n",
            "Processed 20200 rows\n",
            "Processed 20300 rows\n",
            "Processed 20400 rows\n",
            "Processed 20500 rows\n",
            "Processed 20600 rows\n",
            "Processed 20700 rows\n",
            "Processed 20800 rows\n",
            "Processed 20900 rows\n",
            "Processed 21000 rows\n",
            "Processed 21100 rows\n",
            "Processed 21200 rows\n",
            "Processed 21300 rows\n",
            "Processed 21400 rows\n",
            "Processed 21500 rows\n",
            "Processed 21600 rows\n",
            "Processed 21700 rows\n",
            "Processed 21800 rows\n",
            "Processed 21900 rows\n",
            "Processed 22000 rows\n",
            "Processed 22100 rows\n",
            "Processed 22200 rows\n",
            "Processed 22300 rows\n",
            "Processed 22400 rows\n",
            "Processed 22500 rows\n",
            "Processed 22600 rows\n",
            "Processed 22700 rows\n",
            "Processed 22800 rows\n",
            "Audio extraction complete.\n"
          ]
        }
      ],
      "source": [
        "# Create the audio base path if it does not exist\n",
        "os.makedirs(audio_base_path, exist_ok=True)\n",
        "\n",
        "# Load the CSV file\n",
        "df = pd.read_csv(input_csv_path)\n",
        "\n",
        "# Function to extract and save audio using ffmpeg\n",
        "def extract_audio_ffmpeg(video_id, clip_id):\n",
        "    video_path = os.path.join(video_base_path, video_id, f\"{clip_id}.mp4\")\n",
        "    audio_output_dir = os.path.join(audio_base_path, video_id)\n",
        "    os.makedirs(audio_output_dir, exist_ok=True)\n",
        "    audio_output_path = os.path.join(audio_output_dir, f\"{clip_id}.wav\")\n",
        "\n",
        "    # Use ffmpeg to extract audio\n",
        "    command = [\n",
        "        'ffmpeg',\n",
        "        '-i', video_path,\n",
        "        '-vn',  # No video\n",
        "        '-ar', '16000',  # Set audio sample rate to 16kHz\n",
        "        '-ac', '1',  # Set number of audio channels to 1 (mono)\n",
        "        audio_output_path\n",
        "    ]\n",
        "    subprocess.run(command, check=True)\n",
        "\n",
        "# Iterate through the CSV and process each video\n",
        "for index, row in df.iterrows():\n",
        "    video_id = row['video_id']\n",
        "    clip_id = row['clip_id']\n",
        "    extract_audio_ffmpeg(video_id, clip_id)\n",
        "\n",
        "    if index % 100 == 0:\n",
        "        print(f\"Processed {index} rows\")\n",
        "\n",
        "print(\"Audio extraction complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset's given transcripts are noisy, so I need to pre-process them:\n",
        "\n",
        "1. Speaker names would randomly appear at the start of transcrpts, with various formats, so I checked and removed all of them by hand\n",
        "\n",
        "2. Some transcripts are completely wrong, so I need to remove them all.\n",
        "\n",
        "## Remove Wrong Transcripts\n",
        "\n",
        "I'll choose a transcription model that transcribes audio to text. Wav2Vec 2.0 model is a powerful model for speech recognition, which is good for our task.\n",
        "\n",
        "I also find one of its variants: https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-english. This model is fine-tuned for English, and since English is our targeted language, it would likely have a higher accuracy. However, upon testing, I found it 2 to 3 times slower than the original Wav2Vec 2.0. Thus, I'll stick to the original version for time's sake."
      ],
      "metadata": {
        "id": "lfobf__ekJtr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_csv_path = '/content/Capstone-Tang/MOSEI/transcriptions.csv'"
      ],
      "metadata": {
        "id": "3pa0xrZsl50Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
        "import torch\n",
        "import soundfile as sf"
      ],
      "metadata": {
        "id": "BMun63ABlk5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model and processor\n",
        "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "\n",
        "# Function to transcribe audio\n",
        "def transcribe_audio(audio_path):\n",
        "    # Load the audio file\n",
        "    audio_input, sample_rate = sf.read(audio_path)\n",
        "\n",
        "    # Tokenize the audio\n",
        "    input_values = processor(audio_input, return_tensors=\"pt\", padding=\"longest\", sampling_rate=16000).input_values\n",
        "\n",
        "    # Perform inference\n",
        "    with torch.no_grad():\n",
        "        logits = model(input_values).logits\n",
        "\n",
        "    # Decode the logits to get the transcription\n",
        "    predicted_ids = torch.argmax(logits, dim=-1)\n",
        "    transcription = processor.batch_decode(predicted_ids)\n",
        "    return transcription[0]\n",
        "\n",
        "clip_path = '/content/Capstone-Tang/MOSEI/Audio/139006/8.wav'\n",
        "transcription = transcribe_audio(clip_path)\n",
        "print(transcription)"
      ],
      "metadata": {
        "id": "H774iXAHmgwq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Qj_btBMK4k8"
      },
      "source": [
        "# Transcribe Audios"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wWYAzIuMxJF"
      },
      "source": [
        "I'll fine-tune transcription model on the MOSEI model to get a better performance.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6obj-kfNZI6P"
      },
      "source": [
        "First, I'll varify that the model works on my transferred MOSEI audioes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KEjuLkNLmrPS"
      },
      "outputs": [],
      "source": [
        "output_csv_path = '/content/Capstone-Tang/MOSEI/transcriptions.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qaYA-bf78Xck"
      },
      "outputs": [],
      "source": [
        "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
        "import torch\n",
        "import soundfile as sf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQzyAHL4ya80",
        "outputId": "5b9fab7c-ef00-48c6-9e72-ec41585aa858"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at facebook/wav2vec2-base-960h were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
            "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Transcription complete.\n",
            "\n",
            "      video_id clip_id                                      transcription\n",
            "0  -3g5yACwYnA      10  ON KEYS PART OF AH THE PEOPLE THAT WE USE TOTO...\n",
            "1  -3g5yACwYnA      13  THAT WE DO O THEY'VE BEEN ABLE TO FIND SOLUTIO...\n",
            "2  -3g5yACwYnA       3  OM WE'RE A HUGE A USERVE IT HE SAYS FOR OUR OP...\n",
            "3  -3g5yACwYnA       2  ERATIONS AM KEE BRINGS THE KEEP ALROM BRINGS A...\n",
            "4  -3g5yACwYnA       4  KEY BRINGS THOSE TYPES OF A ASPECTS TO OUR BUS...\n"
          ]
        }
      ],
      "source": [
        "# Load model and processor\n",
        "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "\n",
        "# Function to transcribe audio\n",
        "def transcribe_audio(audio_path):\n",
        "    # Load the audio file\n",
        "    audio_input, sample_rate = sf.read(audio_path)\n",
        "\n",
        "    # Tokenize the audio\n",
        "    input_values = processor(audio_input, return_tensors=\"pt\", padding=\"longest\", sampling_rate=16000).input_values\n",
        "\n",
        "    # Perform inference\n",
        "    with torch.no_grad():\n",
        "        logits = model(input_values).logits\n",
        "\n",
        "    # Decode the logits to get the transcription\n",
        "    predicted_ids = torch.argmax(logits, dim=-1)\n",
        "    transcription = processor.batch_decode(predicted_ids)\n",
        "    return transcription[0]\n",
        "\n",
        "transcriptions = []\n",
        "count = 0\n",
        "for video_id in os.listdir(audio_base_path):\n",
        "    video_folder_path = os.path.join(audio_base_path, video_id)\n",
        "    for clip_filename in os.listdir(video_folder_path):\n",
        "        clip_path = os.path.join(video_folder_path, clip_filename)\n",
        "        clip_id = os.path.splitext(clip_filename)[0]\n",
        "        transcription = transcribe_audio(clip_path)\n",
        "        transcriptions.append({\n",
        "            'video_id': video_id,\n",
        "            'clip_id': clip_id,\n",
        "            'transcription': transcription\n",
        "        })\n",
        "        count += 1\n",
        "        if count % 30 == 0:\n",
        "            print(f\"Processed {count} rows\")\n",
        "    # temporary: stop here to see an example\n",
        "    break\n",
        "\n",
        "transcriptions_df = pd.DataFrame(transcriptions)\n",
        "transcriptions_df.to_csv(output_csv_path, index=False)\n",
        "print(\"Transcription complete.\")\n",
        "\n",
        "print(f'\\n{transcriptions_df.head()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHD8Nuy-andC"
      },
      "source": [
        "Next up:\n",
        "1. Transfer all videos to audios\n",
        "2. Fine tune transcription model on MOSEI data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n26CyM0HavGk"
      },
      "source": [
        "# Sentiment Analysis on Transcripts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3TXF8SwcDYy"
      },
      "source": [
        "Although this is future part, I've think of some choices of text-based sentiment analysis model for transfer learning.\n",
        "\n",
        "I would like to go with the BERT family. There are several popular BERT models: RoBERTa, BERT, ALBERT, DistilBERT\n",
        "\n",
        "I'm still trying to decide which one to choose, as better accuracy means more complexity. I have to consider my limited computing resources (I'm on Colab Pro+ right now which is $50 per month)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
