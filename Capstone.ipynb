{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Access and Push"
      ],
      "metadata": {
        "id": "UyuzdRWJuRzd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Access my drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Access github\n",
        "!git clone ###"
      ],
      "metadata": {
        "id": "t75b2qYBy1xA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Git push:"
      ],
      "metadata": {
        "id": "ttpFAcL6zFZF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/Capstone-Tang\n",
        "!git status"
      ],
      "metadata": {
        "id": "OGibWqT6ug_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git add .\n",
        "!git commit -m \"\"  # don't forget commit message\n",
        "!git push origin main"
      ],
      "metadata": {
        "id": "ThVMQ3chzBmN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MOSEI Dataset"
      ],
      "metadata": {
        "id": "PO2VDLNHJdqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the goal of this capstone is to transcribe audios and do sentiment analysis on transcripts, we'll need a multimodal sentiment dataset that contains both audios, transcripts, and sentiment labels.\n",
        "\n",
        "MOSEI is a large-scale dataset with diverse, spontaneous spoken content from online videos. It includes transcriptions, which can be used to fine-tune transcription models. It also includes sentiment and emotion labels, which can be used to train sentiment analysis models on transcripts.\n",
        "\n",
        "The original paper of MOSEI can be found here: https://aclanthology.org/P18-1208/\n",
        "\n",
        "I also considered several other datasets: MELD is limited to the TV show \"Friends\", which may be difficult to generalize; IEMOCAP has limited number of actors and features acted emotions, which may not reflect diverse and spontaneous speech (same for CREMA-D and SAVEE); MOSI is similar to MOSEI, but not as comprehensive."
      ],
      "metadata": {
        "id": "_56atPMDLF3c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transfer Videos to Audios"
      ],
      "metadata": {
        "id": "_tI63wZFJpBr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I tried to access the dataset through its official repo, yet following its instructions results in errors. Then I looked through a range of repos and datasets, gotten several pkl files which contain features for the video and audios of the dataset. I tried to decode them, but there were no instrctions on the form of the feature files, which makes it hard.\n",
        "\n",
        "I finally came across a repo that has a google drive link to the raw videos of the dataset: https://drive.google.com/drive/folders/1o2pOWQg8fxJkgBJVWk9mjCrbcc1jX4eq\n",
        "\n",
        "In this part, I would transfer them to audios. For now, I only transfered 6000 of the videos, and there are more than 22000 videos to transfer."
      ],
      "metadata": {
        "id": "-rzZHRDHKQ6Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import subprocess"
      ],
      "metadata": {
        "id": "DW5L1xCVJ7XC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Paths\n",
        "input_csv_path = '/content/Capstone-Tang/MOSEI/label.csv'\n",
        "video_base_path = '/content/drive/MyDrive/MOSEI/Raw'\n",
        "audio_base_path = '/content/Capstone-Tang/MOSEI/Audio'"
      ],
      "metadata": {
        "id": "ZWyo1ahgJ8Ei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the audio base path if it does not exist\n",
        "os.makedirs(audio_base_path, exist_ok=True)\n",
        "\n",
        "# Load the CSV file\n",
        "df = pd.read_csv(input_csv_path)\n",
        "\n",
        "# Function to extract and save audio using ffmpeg\n",
        "def extract_audio_ffmpeg(video_id, clip_id):\n",
        "    video_path = os.path.join(video_base_path, video_id, f\"{clip_id}.mp4\")\n",
        "    audio_output_dir = os.path.join(audio_base_path, video_id)\n",
        "    os.makedirs(audio_output_dir, exist_ok=True)\n",
        "    audio_output_path = os.path.join(audio_output_dir, f\"{clip_id}.wav\")\n",
        "\n",
        "    # Use ffmpeg to extract audio\n",
        "    command = [\n",
        "        'ffmpeg',\n",
        "        '-i', video_path,\n",
        "        '-vn',  # No video\n",
        "        '-ar', '16000',  # Set audio sample rate to 16kHz\n",
        "        '-ac', '1',  # Set number of audio channels to 1 (mono)\n",
        "        audio_output_path\n",
        "    ]\n",
        "    subprocess.run(command, check=True)\n",
        "\n",
        "# Iterate through the CSV and process each video\n",
        "for index, row in df.iterrows():\n",
        "    video_id = row['video_id']\n",
        "    clip_id = row['clip_id']\n",
        "    extract_audio_ffmpeg(video_id, clip_id)\n",
        "\n",
        "    if index % 30 == 0:\n",
        "        print(f\"Processed {index} rows\")\n",
        "\n",
        "print(\"Audio extraction complete.\")"
      ],
      "metadata": {
        "id": "zFzWIRMnDr-W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fe392b2-7cb2-420c-89d5-99e831547025"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Audio extraction complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transcribe Audios"
      ],
      "metadata": {
        "id": "4Qj_btBMK4k8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next I'll choose a transcription model that transcribes audio to text. I'll fine-tune it on the MOSEI model to get a better performance.\n",
        "\n",
        "Wav2Vec 2.0 model is a powerful model for speech recognition, which is good for our task.\n",
        "\n",
        "I also find one of its variants: https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-english. This model is fine-tuned for English, and since English is our targeted language, it would likely have a higher accuracy. However, upon testing, I found it 2 to 3 times slower than the original Wav2Vec 2.0. Thus, I'll stick to the original version for time's sake."
      ],
      "metadata": {
        "id": "5wWYAzIuMxJF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, I'll varify that the model works on my transferred MOSEI audioes."
      ],
      "metadata": {
        "id": "6obj-kfNZI6P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_csv_path = '/content/Capstone-Tang/MOSEI/transcriptions.csv'"
      ],
      "metadata": {
        "id": "KEjuLkNLmrPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
        "import torch\n",
        "import soundfile as sf"
      ],
      "metadata": {
        "id": "qaYA-bf78Xck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model and processor\n",
        "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "\n",
        "# Function to transcribe audio\n",
        "def transcribe_audio(audio_path):\n",
        "    # Load the audio file\n",
        "    audio_input, sample_rate = sf.read(audio_path)\n",
        "\n",
        "    # Tokenize the audio\n",
        "    input_values = processor(audio_input, return_tensors=\"pt\", padding=\"longest\", sampling_rate=16000).input_values\n",
        "\n",
        "    # Perform inference\n",
        "    with torch.no_grad():\n",
        "        logits = model(input_values).logits\n",
        "\n",
        "    # Decode the logits to get the transcription\n",
        "    predicted_ids = torch.argmax(logits, dim=-1)\n",
        "    transcription = processor.batch_decode(predicted_ids)\n",
        "    return transcription[0]\n",
        "\n",
        "transcriptions = []\n",
        "count = 0\n",
        "for video_id in os.listdir(audio_base_path):\n",
        "    video_folder_path = os.path.join(audio_base_path, video_id)\n",
        "    for clip_filename in os.listdir(video_folder_path):\n",
        "        clip_path = os.path.join(video_folder_path, clip_filename)\n",
        "        clip_id = os.path.splitext(clip_filename)[0]\n",
        "        transcription = transcribe_audio(clip_path)\n",
        "        transcriptions.append({\n",
        "            'video_id': video_id,\n",
        "            'clip_id': clip_id,\n",
        "            'transcription': transcription\n",
        "        })\n",
        "        count += 1\n",
        "        if count % 30 == 0:\n",
        "            print(f\"Processed {count} rows\")\n",
        "    # temporary: stop here to see an example\n",
        "    break\n",
        "\n",
        "transcriptions_df = pd.DataFrame(transcriptions)\n",
        "transcriptions_df.to_csv(output_csv_path, index=False)\n",
        "print(\"Transcription complete.\")\n",
        "\n",
        "print(f'\\n{transcriptions_df.head()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQzyAHL4ya80",
        "outputId": "5b9fab7c-ef00-48c6-9e72-ec41585aa858"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at facebook/wav2vec2-base-960h were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
            "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcription complete.\n",
            "\n",
            "      video_id clip_id                                      transcription\n",
            "0  -3g5yACwYnA      10  ON KEYS PART OF AH THE PEOPLE THAT WE USE TOTO...\n",
            "1  -3g5yACwYnA      13  THAT WE DO O THEY'VE BEEN ABLE TO FIND SOLUTIO...\n",
            "2  -3g5yACwYnA       3  OM WE'RE A HUGE A USERVE IT HE SAYS FOR OUR OP...\n",
            "3  -3g5yACwYnA       2  ERATIONS AM KEE BRINGS THE KEEP ALROM BRINGS A...\n",
            "4  -3g5yACwYnA       4  KEY BRINGS THOSE TYPES OF A ASPECTS TO OUR BUS...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next up:\n",
        "1. Transfer all videos to audios\n",
        "2. Fine tune transcription model on MOSEI data"
      ],
      "metadata": {
        "id": "vHD8Nuy-andC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment Analysis on Transcripts"
      ],
      "metadata": {
        "id": "n26CyM0HavGk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Although this is future part, I've think of some choices of text-based sentiment analysis model for transfer learning.\n",
        "\n",
        "I would like to go with the BERT family. There are several popular BERT models: RoBERTa, BERT, ALBERT, DistilBERT\n",
        "\n",
        "I'm still trying to decide which one to choose, as better accuracy means more complexity. I have to consider my limited computing resources (I'm on Colab Pro+ right now which is $50 per month)."
      ],
      "metadata": {
        "id": "E3TXF8SwcDYy"
      }
    }
  ]
}
